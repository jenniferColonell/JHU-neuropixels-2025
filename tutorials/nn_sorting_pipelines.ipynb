{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509027b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running **spike sorting** pipelines:\n",
    "\n",
    "A spike sorting pipeline for neuropixels should consist of:\n",
    "\n",
    " - preprocessing (phase_shifting, filtering, denoising..)\n",
    " - motion correction\n",
    " - the actual sorting (assigning spikes to units)\n",
    " - waveform extraction\n",
    " - metric computation\n",
    "\n",
    "and some other optional steps like reading __sycnchronization channels__, __manual curation__ and result inspection.\n",
    "\n",
    "### ecephys spike sorting\n",
    "\n",
    "Check the usage instructions [**here**](https://github.com/jenniferColonell/ecephys_spike_sorting#usage).\n",
    "\n",
    "__ecephys__ is installed on the _course computers_; the installation instructions are [**here**](https://github.com/jenniferColonell/ecephys_spike_sorting#installation-with-anaconda-and-kilosort4); don't forget to install _CatGT_ and set the paths in the ``ecephys_spike_sorting\\ecephys_spike_sorting\\scripts\\create_input_json.py`` file if you are installing from scratch. \n",
    "\n",
    "---\n",
    "\n",
    "To **configure** the pipeline to run the test data you need to edit the file ``sglx_multi_run_pipeline.py`` in the ``ecephys_spike_sorting\\ecephys_spike_sorting\\scripts`` directory.\n",
    "\n",
    "In the _course computers_ that file is in: ``C\\Users\\np_user\\Desktop\\software_downloads\\ecephys_course\\ecephys_spike_sorting\\ecephys_spike_sorting\\scripts`` \n",
    "\n",
    "The file (**sglx_multi_run_pipeline.py**) controls most aspects of the pipeline. Some things to check:\n",
    " 1. __line 67__: ``npx_directory = r'<PARENT DIRECTORY OF THE INPUT (e.g. C:\\SGL_DATA)' `` \n",
    " 2. __line 95 to 97__: ``run_specs = [['<FILEPART NAME BEFORE THE GATE WITHOUT UNDERSCORE`, '<GATE NUMBER', '0,0', '0', '0', ['cortex','thalamus','thalamus'] ]``  specifies the file and gate to run, add multiple lines to concatenate gates or sessions\n",
    "\n",
    " 3. __line 105__: ``catGT_dest = r'<PATH TO AN EMPTY FOLDER>'`` don't forget to create the folder. this is the output path.\n",
    " 4. __line 216__: ``json_directory = r'<PATH TO THE OUTPUT WHERE TO STORE JSON FILES>'`` this can be the same folder as point 3 or another if you want the files to be separate.\n",
    " 5. __line 135 to 136__: ``obx_present = True`` set to true to analyze the OneBox channels ``ni_present = False`` to analyze obx channels\n",
    " 6. __line 137__: ``ni_obx_extract_string = '-xa=1,0,1,1,3,50`` extract the 50 ms pulses that mark trial start    \n",
    "\n",
    "---\n",
    "\n",
    "To **run**:\n",
    " - open a terminal where you can run conda commands. \n",
    " - activate the environment ``conda activate ecephys_ks4`` _in the course_.\n",
    " - go to the folder that has the ``sglx_multi_run_pipeline.py`` file using ``cd <FOLDERPATH>``\n",
    " - run the pipeline ``python sglx_multi_run_pipeline.py``\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    " To **visualize** the results:\n",
    "\n",
    "You can visualize with [**phy**](https://github.com/cortex-lab/phy).\n",
    "\n",
    " - open a terminal where you can run conda commands. \n",
    " - activate the environment ``conda activate phy2`` _in the course_.\n",
    " - go to the output folder using ``cd <FOLDERPATH>`` you want the folder that has the **params.py** file.\n",
    " - open phy using the command **``phy template-gui params.py``**\n",
    "\n",
    "\n",
    "You can analyze the visual responses in the notebook 03_visual_responses.ipynb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f77606-a36f-4f89-877e-52152a4573ca",
   "metadata": {},
   "source": [
    "### Combining tools to build a pipeline\n",
    "\n",
    "## Builidng a CatGT command line for preprocessing\n",
    "Preprocessing can also be done in Spikeinterface -- here we use CatGT to handle a few SGLX details more easily.\n",
    "Here we build a CatGT command line to process your data, then look at the result with the SpikeGLX viewer. \n",
    "For more details on CatGT, see the help [**here**](https://billkarsh.github.io/SpikeGLX/CatGT_help/CatGT_ReadMe.html).\n",
    "\n",
    "This is also a general model for incorporating command line or other compiled tools into a simple python notebook or script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea774f8d-bc06-44b1-b753-46fbab2f08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\labadmin\\Desktop\\software_downloads\\ecephys_course/CatGT-win/CatGT.exe -dir=C:\\Users\\labadmin\\Desktop\\test_data -run=SC035_010720_ex -g=0 -t=0,0 -prb_fld -prb=0 -ap -apfilter=butter,12,300,10000 -gblcar -gfix=0.40,0.10,0.02 -maxsecs=600 -out_prb_fld -no_catgt_fld -dest=C:\\Users\\labadmin\\Desktop\\test_data\\output\\SC035_010720_ex_g0\n",
      "CatGT complete, check CatGT.log in notebook folder for errors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os_str='Windows'\n",
    "\n",
    "# set the parent path for the software on your computer\n",
    "software_download_path = r'C:\\Users\\labadmin\\Desktop\\software_downloads'\n",
    "\n",
    "# parent folder of the data\n",
    "data_parent = r'C:\\Users\\labadmin\\Desktop\\test_data'\n",
    "\n",
    "# run name for the acquired data (don't include any gate info)\n",
    "run_name = 'SC035_010720_ex'\n",
    "\n",
    "# gate index (for myrun_g0_t0.imec0.ap.bin, the gate = 0)\n",
    "gate = 0\n",
    "\n",
    "# destination parent\n",
    "dest_parent = r'C:\\Users\\labadmin\\Desktop\\test_data\\output'\n",
    "\n",
    "# time to analyze and sort \n",
    "maxsecs = 600\n",
    "\n",
    "# extraction parameters for stimulus times. parameters: stream type(js), stream index(ip), channel in file, threshold1, threshold2, duration\n",
    "obx_ex_str = '-xa=1,0,1,1,3,50'\n",
    "\n",
    "dest_dir = os.path.join(dest_parent,f'{run_name}_g{gate}')\n",
    "if not os.path.isdir(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "\n",
    "cmd_parts = list()\n",
    "cmd_parts.append(os.path.join(software_download_path,'ecephys_course/CatGT-win/CatGT.exe'))\n",
    "cmd_parts.append(f'-dir={data_parent} -run={run_name} -g={gate} -t=0,0 -prb_fld')\n",
    "cmd_parts.append(f'-prb=0 -ap')\n",
    "cmd_parts.append(f'-apfilter=butter,12,300,10000 -gblcar -gfix=0.40,0.10,0.02')\n",
    "#cmd_parts.append(f'-ob {obx_ex_str}')\n",
    "cmd_parts.append(f'-maxsecs={maxsecs}')\n",
    "cmd_parts.append(f'-out_prb_fld -no_catgt_fld -dest={dest_dir}')\n",
    "                        \n",
    "\n",
    "catGT_cmd = ' '        # use space as the separator for the command parts\n",
    "catGT_cmd = catGT_cmd.join(cmd_parts[1:len(cmd_parts)]) # these are the parameters\n",
    "if os_str=='linux':\n",
    "    # enclose the params in single quotes, so curly braces will not be interpreted by Linux\n",
    "    catGT_cmd = f\"{cmd_parts[0]} '{catGT_cmd}'\"\n",
    "else:\n",
    "    catGT_cmd = f\"{cmd_parts[0]} {catGT_cmd}\"\n",
    "\n",
    "print(catGT_cmd)\n",
    "subprocess.Popen(catGT_cmd,shell='False').wait()\n",
    "print('CatGT complete, check CatGT.log in notebook folder for errors.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06089076",
   "metadata": {},
   "source": [
    "### Running Spike interface on preprocessed data\n",
    "\n",
    "The processed data will be saved to: ``<destination parent>/<run_name>_g<gate>``. Open it up in the SpikeGLX viewer and look for any problems. Is the background subtraction sufficient? Are any major artifacts removed?\n",
    "\n",
    "After checking that the preprocessed data looks OK, move to sorting a single shank, using spike interface\n",
    "\n",
    "\n",
    "Check the instructions [**here**](https://spikeinterface.readthedocs.io/en/0.93.0/overview.html).\n",
    "\n",
    "__spikeinterface__ is installed in the npix-analysis environment on the _course computers_; the installation instructions are [**here**](https://spikeinterface.readthedocs.io/en/0.93.0/installation.html).\n",
    "\n",
    "Be sure to checkout the other [tutorials](https://spikeinterface.readthedocs.io/en/0.93.0/getting_started/plot_getting_started.html#).\n",
    "\n",
    "---\n",
    "\n",
    "We will build and run a pipeline on this notebook, it is based on the [how to guide](https://spikeinterface.readthedocs.io/en/latest/how_to/analyze_neuropixels.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "733935a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are streams ['imec0.ap', 'imec0.ap-SYNC'] in folder C:\\Users\\labadmin\\Desktop\\test_data\\output\\SC035_010720_ex_g0\n"
     ]
    }
   ],
   "source": [
    "import spikeinterface.full as si\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# parameters -- these only pertain to the sorter, because the data is already preprocessed.\n",
    "# in this example, we keep the filtering and whitening in Kilosort 4. This is to keep the native Ks4 whitening (to avoid scaling problems)\n",
    "\n",
    "ks4_params = si.get_default_sorter_params('kilosort4')\n",
    "ks4_params['do_CAR'] = False # skip CAR in kilosort\n",
    "job_kwargs = dict(n_jobs=-1, chunk_duration='1s', progress_bar=True) # how to chunk and process data\n",
    "\n",
    "sort_folder = os.path.join(dest_dir,f'ks4_out')  # this folder is created by KS4; existing folder cannot be overwritten\n",
    "\n",
    "# load spikeglx data, pointing to the destination folder created above\n",
    "stream_names, stream_ids = si.get_neo_streams('spikeglx', dest_dir)\n",
    "\n",
    "print(f'There are streams {stream_names} in folder {dest_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8ac1f37-dc90-4ffb-812d-8f13caef905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET the stream parameter for the file you want to sort here\n",
    "\n",
    "selected_stream = 'imec0.ap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f66e55cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Kilosort4 parameters are: \n",
      "   - batch_size: 60000\n",
      "   - nblocks: 1\n",
      "   - Th_universal: 9\n",
      "   - Th_learned: 8\n",
      "   - nt: 61\n",
      "   - shift: None\n",
      "   - scale: None\n",
      "   - batch_downsampling: 1\n",
      "   - artifact_threshold: inf\n",
      "   - nskip: 25\n",
      "   - whitening_range: 32\n",
      "   - highpass_cutoff: 300\n",
      "   - binning_depth: 5\n",
      "   - sig_interp: 20\n",
      "   - drift_smoothing: [0.5, 0.5, 0.5]\n",
      "   - nt0min: None\n",
      "   - dmin: None\n",
      "   - dminx: 32\n",
      "   - min_template_size: 10\n",
      "   - template_sizes: 5\n",
      "   - nearest_chans: 10\n",
      "   - nearest_templates: 100\n",
      "   - max_channel_distance: 32\n",
      "   - max_peels: 100\n",
      "   - templates_from_data: True\n",
      "   - n_templates: 6\n",
      "   - n_pcs: 6\n",
      "   - Th_single_ch: 6\n",
      "   - acg_threshold: 0.2\n",
      "   - ccg_threshold: 0.25\n",
      "   - cluster_neighbors: 10\n",
      "   - cluster_downsampling: 20\n",
      "   - max_cluster_subset: 25000\n",
      "   - x_centers: None\n",
      "   - cluster_init_seed: 5\n",
      "   - duplicate_spike_ms: 0.25\n",
      "   - position_limit: 100\n",
      "   - do_CAR: False\n",
      "   - invert_sign: False\n",
      "   - save_extra_vars: False\n",
      "   - save_preprocessed_copy: False\n",
      "   - torch_device: auto\n",
      "   - bad_channels: None\n",
      "   - clear_cache: False\n",
      "   - do_correction: True\n",
      "   - skip_kilosort_preprocessing: False\n",
      "   - keep_good_only: False\n",
      "   - use_binary_file: True\n",
      "   - delete_recording_dat: True\n",
      "   - pool_engine: process\n",
      "   - n_jobs: 1\n",
      "   - chunk_duration: 1s\n",
      "   - progress_bar: True\n",
      "   - mp_context: None\n",
      "   - max_threads_per_worker: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'All Kilosort4 parameters are: ')\n",
    "for k in ks4_params.keys():\n",
    "    print(f'   - {k}: {ks4_params[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "093f0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02c1b1d35b84caa9d3591dce83e939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "write_binary_recording (no parallelization):   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Computing preprocessing variables.\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.run_kilosort: N samples: 3599937\n",
      "kilosort.run_kilosort: N seconds: 119.99998666776817\n",
      "kilosort.run_kilosort: N batches: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping common average reference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kilosort.run_kilosort: Preprocessing filters computed in 0.98s; total 0.98s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after preprocessing\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:     1.50 %\n",
      "kilosort.run_kilosort: Mem used:     24.20 %     |      30.93 GB\n",
      "kilosort.run_kilosort: Mem avail:    96.99 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.08 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:    11.88 %     |      1.31   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Computing drift correction.\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.spikedetect: Re-computing universal templates from data.\n",
      "kilosort.spikedetect: Number of universal templates: 1140\n",
      "kilosort.spikedetect: Detecting spikes...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  2.00it/s]\n",
      "kilosort.run_kilosort: drift computed in 32.22s; total 35.08s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after drift correction\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:    50.00 %\n",
      "kilosort.run_kilosort: Mem used:     22.90 %     |      29.23 GB\n",
      "kilosort.run_kilosort: Mem avail:    98.69 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.09 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:    23.82 %     |      2.62   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Extracting spikes using templates\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.spikedetect: Re-computing universal templates from data.\n",
      "kilosort.spikedetect: Number of universal templates: 1140\n",
      "kilosort.spikedetect: Detecting spikes...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:25<00:00,  2.39it/s]\n",
      "kilosort.run_kilosort: 212304 spikes extracted in 26.10s; total 61.22s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after spike detect (univ)\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:     0.00 %\n",
      "kilosort.run_kilosort: Mem used:     22.80 %     |      29.23 GB\n",
      "kilosort.run_kilosort: Mem avail:    98.69 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.09 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:    23.83 %     |      2.62   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: First clustering\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:56<00:00, 14.06s/it]\n",
      "kilosort.run_kilosort: 464 clusters found, in 57.33s; total 118.60s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after first clustering\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:    50.00 %\n",
      "kilosort.run_kilosort: Mem used:     22.90 %     |      29.31 GB\n",
      "kilosort.run_kilosort: Mem avail:    98.61 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.12 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:     4.90 %     |      0.54   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Extracting spikes using cluster waveforms\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:15<00:00,  3.83it/s]\n",
      "kilosort.run_kilosort: 340695 spikes extracted in 15.80s; total 134.45s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after spike detect (learned)\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:     0.00 %\n",
      "kilosort.run_kilosort: Mem used:     23.10 %     |      29.49 GB\n",
      "kilosort.run_kilosort: Mem avail:    98.43 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.12 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:    13.31 %     |      1.46   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Final clustering\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:57<00:00, 14.27s/it]\n",
      "kilosort.run_kilosort: 423 clusters found, in 57.11s; total 191.61s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Merging clusters\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.run_kilosort: 408 units found, in 1.87s; total 193.50s\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after clustering\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:    52.50 %\n",
      "kilosort.run_kilosort: Mem used:     23.00 %     |      29.39 GB\n",
      "kilosort.run_kilosort: Mem avail:    98.53 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.09 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:     0.33 %     |      0.04   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Saving to phy and computing refractory periods\n",
      "kilosort.run_kilosort: ----------------------------------------\n",
      "kilosort.run_kilosort: 254 units found with good refractory periods\n",
      "kilosort.run_kilosort: Exporting to Phy took: 4.99s\n",
      "kilosort.run_kilosort: Total runtime: 198.54s = 00:03:19 h:m:s\n",
      "kilosort.run_kilosort: Sorting output saved in: C:\\Users\\labadmin\\Desktop\\test_data\\output\\SC035_010720_ex_g0\\ks4_out_v3\\sorter_output.\n",
      "kilosort.run_kilosort:  \n",
      "kilosort.run_kilosort: Resource usage after saving\n",
      "kilosort.run_kilosort: ********************************************************\n",
      "kilosort.run_kilosort: CPU usage:    11.20 %\n",
      "kilosort.run_kilosort: Mem used:     23.20 %     |      29.64 GB\n",
      "kilosort.run_kilosort: Mem avail:    98.28 / 127.92 GB\n",
      "kilosort.run_kilosort: ------------------------------------------------------\n",
      "kilosort.run_kilosort: GPU usage:    `conda install pynvml` for GPU usage\n",
      "kilosort.run_kilosort: GPU memory:   50.42 %     |      5.55   /    11.00 GB\n",
      "kilosort.run_kilosort: Allocated:     0.09 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: Max alloc:     0.10 %     |      0.01   /    11.00 GB\n",
      "kilosort.run_kilosort: ********************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kilosort4 run time 198.94s\n"
     ]
    }
   ],
   "source": [
    "# sort the stream for the selected shank\n",
    "\n",
    "rec = si.read_spikeglx(dest_dir, stream_name=selected_stream, load_sync_channel=False)\n",
    "\n",
    "# run ks4\n",
    "sorting = si.run_sorter('kilosort4', rec, folder=sort_folder,\n",
    "                        docker_image=False, verbose=True, **ks4_params)\n",
    "sorting = si.read_sorter_folder(sort_folder)\n",
    "\n",
    "# phy output from ks4 is saved to sort_folder/sorter_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "318cf681-bb96-4f14-8f33-74a7569fe9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b647804aadbd47aa8f41617f46780803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "estimate_sparsity (no parallelization):   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e51c36ba336404c84097852d5afc225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compute_waveforms (workers: 40 processes):   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef05452c4414251a66b5dbc52af6b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "noise_level (no parallelization):   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed86b257445472ebaf40ca8420c98bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spike_amplitudes (workers: 40 processes):   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\labadmin\\.conda\\envs\\npix-new\\Lib\\site-packages\\spikeinterface\\qualitymetrics\\misc_metrics.py:1059: UserWarning: Some units have too few spikes : amplitude_cutoff is set to NaN\n",
      "  warnings.warn(f\"Some units have too few spikes : amplitude_cutoff is set to NaN\")\n"
     ]
    }
   ],
   "source": [
    "# calculate average waveforms and metrics.\n",
    "\n",
    "sorting = si.read_sorter_folder(sort_folder)\n",
    "\n",
    "analyzer = si.create_sorting_analyzer(sorting, rec, sparse=True, format=\"memory\")\n",
    "# compute waveforms \n",
    "analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n",
    "analyzer.compute(\"waveforms\",  ms_before=1.5, ms_after=1.5, **job_kwargs)\n",
    "analyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\n",
    "analyzer.compute(\"noise_levels\")\n",
    "analyzer.compute(\"correlograms\")\n",
    "analyzer.compute(\"unit_locations\")\n",
    "analyzer.compute(\"spike_amplitudes\", **job_kwargs)\n",
    "\n",
    "# quality metrics\n",
    "metric_names=['firing_rate', 'presence_ratio', 'snr', 'isi_violation', 'amplitude_cutoff']\n",
    "metrics = si.compute_quality_metrics(analyzer, metric_names=metric_names)\n",
    "\n",
    "# all of the analyzer computed quantitites are saved in sort_folder/analyzer/extensions\n",
    "analyzer_saved = analyzer.save_as(folder=os.path.join(sort_folder, \"analyzer\"), format=\"binary_folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c2312-0a98-4e06-9265-ac346e1c69d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
